{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZilferII/CodeVault/blob/main/AutoGPT_stocks4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import yaml\n",
        "\n",
        "def stop_services(ports):\n",
        "    for port in ports:\n",
        "        try:\n",
        "            pid = subprocess.check_output(f\"lsof -t -i:{port}\", shell=True).decode().strip()\n",
        "            if pid:\n",
        "                os.system(f\"kill -9 {pid}\")\n",
        "                print(f\"Service running on port {port} stopped successfully.\")\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"No service running on port {port}.\")\n",
        "\n",
        "def clear_temporary_files(temp_dirs):\n",
        "    for temp_dir in temp_dirs:\n",
        "        try:\n",
        "            shutil.rmtree(temp_dir)\n",
        "            print(f\"Temporary directory {temp_dir} cleared.\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Temporary directory {temp_dir} not found.\")\n",
        "\n",
        "def delete_generated_files(files):\n",
        "    for file in files:\n",
        "        try:\n",
        "            os.remove(file)\n",
        "            print(f\"File {file} deleted.\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File {file} not found.\")\n",
        "\n",
        "def reset_environment_vars(env_vars):\n",
        "    for var in env_vars:\n",
        "        if var in os.environ:\n",
        "            del os.environ[var]\n",
        "            print(f\"Environment variable {var} reset.\")\n",
        "def clean_server():\n",
        "    # Define the ports to stop services on\n",
        "    ports = [8000]\n",
        "\n",
        "    # Define the temporary directories to clear\n",
        "    temp_dirs = ['__pycache__', 'tmp', 'logs']\n",
        "\n",
        "    # Define the generated files to delete\n",
        "    files = ['trained_lstm_model.h5', 'historical_data.csv', 'real_time_market_data.csv']\n",
        "\n",
        "    # Define environment variables to reset\n",
        "    env_vars = ['ALPACA_API_KEY', 'ALPACA_SECRET_KEY', 'ALPACA_ENDPOINT', 'NEWS_API_KEY', 'FMP_API_KEY']\n",
        "\n",
        "\n",
        "\n",
        "    stop_services(ports)\n",
        "    clear_temporary_files(temp_dirs)\n",
        "    delete_generated_files(files)\n",
        "    reset_environment_vars(env_vars)"
      ],
      "metadata": {
        "id": "qng81HUDVPM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Delete existing config files and directories if they exist\n",
        "config_paths = [\n",
        "    \"/root/.config/ngrok/ngrok.yml\",\n",
        "    \"/root/.ngrok2/ngrok.yml\"\n",
        "]\n",
        "\n",
        "for path in config_paths:\n",
        "    if os.path.exists(path):\n",
        "        if os.path.isfile(path):\n",
        "            os.remove(path)\n",
        "            print(f\"Deleted existing config file: {path}\")\n",
        "        elif os.path.isdir(path):\n",
        "            shutil.rmtree(path)\n",
        "            print(f\"Deleted existing config directory: {path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-dsWw5w6yN8",
        "outputId": "4c66c0df-f001-4fad-c042-7de5fea0399e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted existing config file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile unbind_port.py\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "def unbind_port(port):\n",
        "    try:\n",
        "        pid = subprocess.check_output(f\"lsof -t -i:{port}\", shell=True).decode().strip()\n",
        "        if pid:\n",
        "            os.system(f\"kill -9 {pid}\")\n",
        "            print(f\"Port {port} unbound successfully.\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"Port {port} is already free.\")\n",
        "\n",
        "unbind_port(8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFtbe9dOPhUG",
        "outputId": "5acbe688-2774-4740-c49d-f76884dcab58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting unbind_port.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y build-essential wget libncurses5-dev libncursesw5-dev\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok"
      ],
      "metadata": {
        "id": "Okx-w1Aapy5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "50f369ee-8fc2-42c8-c3a2-1d4dc75befde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 https://ngrok-agent.s3.amazonaws.com buster InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libncurses5-dev is already the newest version (6.3-2ubuntu0.1).\n",
            "libncursesw5-dev is already the newest version (6.3-2ubuntu0.1).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n",
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 https://ngrok-agent.s3.amazonaws.com buster InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "46 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ngrok is already the newest version (3.12.0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install alpaca-trade-api alpha_vantage tensorflow scikit-learn matplotlib dash dash-core-components dash-html-components dash-bootstrap-components fastapi uvicorn nest-asyncio pyngrok transformers plotly pandas requests backtrader boto3 keras-tuner deap"
      ],
      "metadata": {
        "id": "28gbk92sp3f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d865e99f-c273-4279-8fb2-ffb382c954d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: alpaca-trade-api in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: alpha_vantage in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: dash in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: dash-core-components in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: dash-html-components in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: dash-bootstrap-components in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.111.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.30.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.6)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (5.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: backtrader in /usr/local/lib/python3.10/dist-packages (1.9.78.123)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.34.136)\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: deap in /usr/local/lib/python3.10/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api) (1.25.2)\n",
            "Requirement already satisfied: urllib3<2,>1.24 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api) (1.26.19)\n",
            "Requirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api) (1.8.0)\n",
            "Requirement already satisfied: websockets<11,>=9.0 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api) (10.4)\n",
            "Requirement already satisfied: msgpack==1.0.3 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api) (1.0.3)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api) (3.9.5)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api) (6.0.1)\n",
            "Requirement already satisfied: deprecation==2.1.0 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deprecation==2.1.0->alpaca-trade-api) (24.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.10/dist-packages (from dash) (3.0.3)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash) (5.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash) (7.2.0)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.10/dist-packages (from dash) (1.3.4)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.37.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.7.4)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.27.0)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.1.4)\n",
            "Requirement already satisfied: python-multipart>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.0.9)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (5.10.0)\n",
            "Requirement already satisfied: orjson>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.10.5)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.2.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly) (8.4.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.136 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.34.136)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.10.2)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api) (4.0.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi) (2.6.1)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.2->fastapi) (0.12.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (2.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.22.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash) (3.19.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->fastapi) (1.2.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (13.7.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['ALPACA_API_KEY'] = 'PKDEDZT5O8CI3R6UTONM'\n",
        "os.environ['ALPACA_SECRET_KEY'] = 'aPHjNjSp7d6gfDeUY1aElMc92iijlvcMUrnlTIUr'\n",
        "os.environ['ALPACA_ENDPOINT'] = 'https://paper-api.alpaca.markets'\n",
        "os.environ['NEWS_API_KEY'] = '8cc36860da3d4624810890fe4feb78e3'\n",
        "os.environ['FMP_API_KEY'] = 'efCfJ6ItVI48a16QgQrc8WxH4BORgQTa'"
      ],
      "metadata": {
        "id": "3nu8avHoqAla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the paths for the ngrok configuration file\n",
        "xdg_config_path = os.path.expanduser(\"~/.config/ngrok/ngrok.yml\")\n",
        "\n",
        "\n",
        "# Define the content of the configuration file\n",
        "ngrok_config_content = \"\"\"\n",
        "authtoken: 2iXBVyuqMaMjHyK44ixjAcouHKe_5A2z1GqFXjoaST4Yh7GDQ\n",
        "tunnels:\n",
        "  myservice:\n",
        "    addr: 8000\n",
        "    proto: http\n",
        "\"\"\"\n",
        "\n",
        "# Ensure the directories exist and write the configuration file\n",
        "for config_path in [xdg_config_path]:\n",
        "    config_dir = os.path.dirname(config_path)\n",
        "    if not os.path.exists(config_dir):\n",
        "        os.makedirs(config_dir)\n",
        "    with open(config_path, 'w') as config_file:\n",
        "        config_file.write(ngrok_config_content)\n",
        "\n",
        "print(f\"ngrok configuration file created at {xdg_config_path} and {legacy_config_path}\")"
      ],
      "metadata": {
        "id": "y1DwMWuIFns2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_streams.py\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from queue import Queue\n",
        "from threading import Thread\n",
        "import time\n",
        "\n",
        "ALPACA_API_KEY = os.getenv('ALPACA_API_KEY')\n",
        "ALPACA_SECRET_KEY = os.getenv('ALPACA_SECRET_KEY')\n",
        "ALPACA_ENDPOINT = os.getenv('ALPACA_ENDPOINT', 'https://paper-api.alpaca.markets')\n",
        "FMP_API_KEY = os.getenv('FMP_API_KEY')\n",
        "\n",
        "data_queue = Queue()\n",
        "\n",
        "def fetch_data(endpoint, params={}):\n",
        "    base_url = f\"https://financialmodelingprep.com/api\"\n",
        "    url = f\"{base_url}{endpoint}?apikey={FMP_API_KEY}\"\n",
        "    for key, value in params.items():\n",
        "        url += f\"&{key}={value}\"\n",
        "    response = requests.get(url)\n",
        "    return response.json()\n",
        "\n",
        "def normalize_data(data):\n",
        "    if isinstance(data, list):\n",
        "        return pd.DataFrame(data)\n",
        "    elif isinstance(data, dict) and 'data' in data:\n",
        "        return pd.DataFrame(data['data'])\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def fetch_alpaca_data(symbol):\n",
        "    import alpaca_trade_api as tradeapi\n",
        "    api = tradeapi.REST(ALPACA_API_KEY, ALPACA_SECRET_KEY, ALPACA_ENDPOINT, api_version='v2')\n",
        "    bars = api.get_bars(symbol, '1D', limit=100)\n",
        "    return [{'time': bar.t.isoformat(), 'open': bar.o, 'high': bar.h, 'low': bar.l, 'close': bar.c, 'volume': bar.v} for bar in bars]\n",
        "\n",
        "def fetch_real_time_data():\n",
        "    while True:\n",
        "        combined_data = {\n",
        "            \"stock_quote\": fetch_data('/v3/quote/AAPL'),\n",
        "            \"historical_prices\": fetch_alpaca_data('AAPL'),\n",
        "            \"news\": fetch_data('/v3/stock_news', {'tickers': 'AAPL', 'limit': 50}),\n",
        "            \"social_sentiment\": fetch_data('/v4/historical/social-sentiment', {'symbol': 'AAPL', 'page': 0}),\n",
        "        }\n",
        "        normalized_data = {key: normalize_data(value) for key, value in combined_data.items()}\n",
        "        data_queue.put(normalized_data)\n",
        "        time.sleep(60)"
      ],
      "metadata": {
        "id": "Pl3seASTrMei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde208d1-d9c3-4dc0-d2b3-77ee69420d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting data_streams.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocessing.py\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from feature_engineering import extract_features, analyze_sentiments\n",
        "\n",
        "def preprocess_data(data):\n",
        "    features = extract_features(data)\n",
        "    features = analyze_sentiments(features)\n",
        "\n",
        "    historical_prices = pd.DataFrame(features['historical_prices'])\n",
        "    social_sentiment = pd.DataFrame(features['social_sentiment'])\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    historical_prices['scaled_close'] = scaler.fit_transform(historical_prices[['close']])\n",
        "\n",
        "    merged_data = historical_prices.merge(social_sentiment, left_on='time', right_on='date', how='left')\n",
        "    merged_data.fillna(method='ffill', inplace=True)\n",
        "    return merged_data, scaler\n",
        "\n",
        "def create_lstm_dataset(data, window_size=60):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - window_size - 1):\n",
        "        X.append(data[i:(i + window_size), 0])\n",
        "        y.append(data[i + window_size, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def preprocess_for_random_forest(data):\n",
        "    data = data.dropna()\n",
        "    X = data.drop(columns=['target']).values\n",
        "    y = data['target'].values\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "u2dZyBX4rU9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8da4e6a-f2b9-4829-bcb1-88f7a2b076c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting preprocessing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile feature_engineering.py\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "def extract_features(data):\n",
        "    features = []\n",
        "    for key, df in data.items():\n",
        "        if key == 'historical_prices':\n",
        "            df['close'] = df['close'].astype(float)\n",
        "            features.extend(df[['time', 'close']].to_dict('records'))\n",
        "        elif key == 'social_sentiment':\n",
        "            features.extend(df[['date', 'sentiment']].to_dict('records'))\n",
        "    return features\n",
        "\n",
        "def analyze_sentiments(features):\n",
        "    sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
        "    for feature in features:\n",
        "        if 'sentiment' in feature:\n",
        "            sentiment_result = sentiment_analysis(feature['sentiment'])[0]\n",
        "            feature['sentiment_analysis'] = sentiment_result['label']\n",
        "    return features"
      ],
      "metadata": {
        "id": "WK1iQ4HZsExx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b5c71b-7d32-4181-80f5-a6f1ca27a9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting feature_engineering.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_preparation.py\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def build_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=(input_shape, 1)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=50, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=25))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "def train_lstm_model(X_train, y_train, X_val, y_val):\n",
        "    model = build_lstm_model(X_train.shape[1])\n",
        "    model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=1)\n",
        "    return model\n",
        "\n",
        "def build_rl_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=input_shape, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(2, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "def train_rl_model(model, data, epochs=100):\n",
        "    for epoch in range(epochs):\n",
        "        state = data[0]\n",
        "        for t in range(len(data) - 1):\n",
        "            action = np.argmax(model.predict(state.reshape(1, -1)))\n",
        "            next_state = data[t + 1]\n",
        "            reward = get_reward(next_state, action)\n",
        "            target = reward + 0.99 * np.max(model.predict(next_state.reshape(1, -1)))\n",
        "            target_f = model.predict(state.reshape(1, -1))\n",
        "            target_f[0][action] = target\n",
        "            model.fit(state.reshape(1, -1), target_f, epochs=1, verbose=0)\n",
        "            state = next_state\n",
        "\n",
        "def get_reward(state, action):\n",
        "    price_change = state[1] - state[0]\n",
        "    if action == 1:  # Buy action\n",
        "        return price_change if price_change > 0 else -1\n",
        "    elif action == 0:  # Sell action\n",
        "        return -price_change if price_change < 0 else -1\n",
        "    return -1"
      ],
      "metadata": {
        "id": "M4NMFO49sPK4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2982ab82-9a4d-460f-9959-a76d541eaf2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model_preparation.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pattern_recognition.py\n",
        "import talib\n",
        "import pandas as pd\n",
        "\n",
        "def recognize_patterns(data):\n",
        "    patterns = []\n",
        "    open_prices = data['open'].values\n",
        "    high_prices = data['high'].values\n",
        "    low_prices = data['low'].values\n",
        "    close_prices = data['close'].values\n",
        "\n",
        "    engulfing = talib.CDLENGULFING(open_prices, high_prices, low_prices, close_prices)\n",
        "\n",
        "    for i in range(len(engulfing)):\n",
        "        if engulfing[i] == 100:  # Bullish engulfing\n",
        "            patterns.append({\n",
        "                'symbol': data['symbol'].iloc[i],\n",
        "                'pattern': 'bullish_engulfing',\n",
        "                'confidence': 0.9,\n",
        "                'index': i\n",
        "            })\n",
        "        elif engulfing[i] == -100:  # Bearish engulfing\n",
        "            patterns.append({\n",
        "                'symbol': data['symbol'].iloc[i],\n",
        "                'pattern': 'bearish_engulfing',\n",
        "                'confidence': 0.9,\n",
        "                'index': i\n",
        "            })\n",
        "    return patterns"
      ],
      "metadata": {
        "id": "qdGzwGEesem8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56aff9b2-b94c-4ae8-92f5-302a3a9b87ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting pattern_recognition.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile trade_execution.py\n",
        "import alpaca_trade_api as tradeapi\n",
        "import os\n",
        "import time\n",
        "\n",
        "ALPACA_API_KEY = os.getenv('ALPACA_API_KEY')\n",
        "ALPACA_SECRET_KEY = os.getenv('ALPACA_SECRET_KEY')\n",
        "ALPACA_ENDPOINT = os.getenv('ALPACA_ENDPOINT', 'https://paper-api.alpaca.markets')\n",
        "\n",
        "api = tradeapi.REST(ALPACA_API_KEY, ALPACA_SECRET_KEY, ALPACA_ENDPOINT, api_version='v2')\n",
        "\n",
        "def execute_trade(action, symbol, qty):\n",
        "    try:\n",
        "        if action == 'buy':\n",
        "            api.submit_order(\n",
        "                symbol=symbol,\n",
        "                qty=qty,\n",
        "                side='buy',\n",
        "                type='market',\n",
        "                time_in_force='gtc'\n",
        "            )\n",
        "        elif action == 'sell':\n",
        "            api.submit_order(\n",
        "                symbol=symbol,\n",
        "                qty=qty,\n",
        "                side='sell',\n",
        "                type='market',\n",
        "                time_in_force='gtc'\n",
        "            )\n",
        "        print(f\"Executed {action} trade for {symbol} ({qty} shares).\")\n",
        "    except Exception as e:\n",
        "        print(f\"Trade execution failed: {e}\")\n",
        "        time.sleep(5)\n",
        "        execute_trade(action, symbol, qty)  # Retry once after a short delay"
      ],
      "metadata": {
        "id": "zHaS-HaUsnXV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb448ce-934a-4c25-e78e-09913bea74a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting trade_execution.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile visualization.py\n",
        "from dash import dcc, html, Dash\n",
        "from dash.dependencies import Input, Output, State\n",
        "import pandas as pd\n",
        "from threading import Thread\n",
        "import plotly.graph_objs as go\n",
        "from data_streams import data_queue\n",
        "from trade_execution import execute_trade\n",
        "from preprocessing import preprocess_data, create_lstm_dataset\n",
        "from model_preparation import build_lstm_model, train_lstm_model\n",
        "\n",
        "dash_app = Dash(__name__)\n",
        "\n",
        "dash_app.layout = html.Div([\n",
        "    html.H1(\"AutoGPT Trader Dashboard\"),\n",
        "    dcc.Dropdown(\n",
        "        id='stock-selector',\n",
        "        options=[{'label': 'AAPL', 'value': 'AAPL'}, {'label': 'GOOG', 'value': 'GOOG'}, {'label': 'TSLA', 'value': 'TSLA'}],\n",
        "        value='AAPL'\n",
        "    ),\n",
        "    dcc.Dropdown(\n",
        "        id='risk-level',\n",
        "        options=[{'label': 'Low', 'value': 1}, {'label': 'Medium', 'value': 2}, {'label': 'High', 'value': 3}],\n",
        "        value=2\n",
        "    ),\n",
        "    dcc.Graph(id='price-chart'),\n",
        "    html.Button('Run Model', id='run-model', n_clicks=0),\n",
        "    html.Button('Backtest', id='backtest-button', n_clicks=0),\n",
        "    html.Button('Train Sharpe', id='sharpe-train-button', n_clicks=0),\n",
        "    html.Div(id='trade-output'),\n",
        "    html.Div(id='backtest-output'),\n",
        "    html.Div(id='sharpe-output'),\n",
        "    html.Div([\n",
        "        html.Label('Alpaca API Key:'),\n",
        "        dcc.Input(id='api-key', type='text', value=''),\n",
        "        html.Label('Alpaca API Secret:'),\n",
        "        dcc.Input(id='api-secret', type='password', value=''),\n",
        "        html.Label('Alpaca Endpoint:'),\n",
        "        dcc.Input(id='api-endpoint', type='text', value=''),\n",
        "        html.Button('Update API Keys', id='update-api-button', n_clicks=0),\n",
        "        html.Div(id='update-status')\n",
        "    ])\n",
        "])\n",
        "\n",
        "@dash_app.callback(\n",
        "    [Output('price-chart', 'figure'), Output('trade-output', 'children')],\n",
        "    [Input('run-model', 'n_clicks'), Input('stock-selector', 'value'), Input('risk-level', 'value')]\n",
        ")\n",
        "def update_graph(n_clicks, stock, risk_level):\n",
        "    if n_clicks > 0:\n",
        "        if not data_queue.empty():\n",
        "            data = data_queue.get()\n",
        "            preprocessed_data, scaler = preprocess_data(data)\n",
        "            X, y = create_lstm_dataset(preprocessed_data['scaled_close'].values.reshape(-1, 1))\n",
        "            lstm_model = build_lstm_model(X.shape[1])\n",
        "            lstm_model.fit(X, y, epochs=50, verbose=1)\n",
        "            predictions = lstm_model.predict(X)\n",
        "            trades = [{'symbol': stock, 'profit_margin': 0.05, 'risk': 50}]\n",
        "            execute_trade('buy', stock, 10)  # Replace with actual trading logic\n",
        "            fig = {\n",
        "                'data': [{'x': preprocessed_data['time'], 'y': preprocessed_data['close'], 'type': 'line'}],\n",
        "                'layout': {'title': f'Historical Prices for {stock}'}\n",
        "            }\n",
        "            return fig, \"Trade executed based on model predictions and risk level\"\n",
        "        return {}, \"No data available\"\n",
        "    return {}, \"Click 'Run Model' to execute trading logic\"\n",
        "\n",
        "@dash_app.callback(\n",
        "    Output('backtest-output', 'children'),\n",
        "    [Input('backtest-button', 'n_clicks')],\n",
        "    [Input('stock-selector', 'value')]\n",
        ")\n",
        "def backtest_strategy(n_clicks, stock):\n",
        "    if n_clicks > 0:\n",
        "        backtest_results = perform_backtest(stock, '2020-01-01', '2021-01-01')\n",
        "        return f\"Backtest Results for {stock}: {backtest_results}\"\n",
        "    return \"Click 'Backtest' to run the strategy\"\n",
        "\n",
        "def perform_backtest(symbol, start_date, end_date):\n",
        "    cerebro = bt.Cerebro()\n",
        "    cerebro.addstrategy(LSTMStrategy)\n",
        "    historical_prices = fetch_historical_data(symbol, start_date, end_date)\n",
        "    df = pd.DataFrame(historical_prices)\n",
        "    data = bt.feeds.PandasData(dataname=df)\n",
        "    cerebro.adddata(data)\n",
        "    cerebro.broker.set_cash(100000)\n",
        "    cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe_ratio')\n",
        "    results = cerebro.run()\n",
        "    strat = results[0]\n",
        "    sharpe_ratio = strat.analyzers.sharpe_ratio.get_analysis()['sharperatio']\n",
        "    return f'Sharpe Ratio: {sharpe_ratio}'\n",
        "\n",
        "@dash_app.callback(\n",
        "    Output('sharpe-output', 'children'),\n",
        "    [Input('sharpe-train-button', 'n_clicks')],\n",
        "    [Input('stock-selector', 'value')]\n",
        ")\n",
        "def train_sharpe_strategy(n_clicks, stock):\n",
        "    if n_clicks > 0:\n",
        "        training_results = train_sharpe(stock)\n",
        "        return f\"Sharpe Training Results for {stock}: {training_results}\"\n",
        "    return \"Click 'Train Sharpe' to start the training\"\n",
        "\n",
        "def train_sharpe(stock):\n",
        "    historical_prices = fetch_historical_data(stock, '2020-01-01', '2021-01-01')\n",
        "    df = pd.DataFrame(historical_prices)\n",
        "    returns = df['close'].pct_change().dropna()\n",
        "    mean_return = returns.mean()\n",
        "    std_return = returns.std()\n",
        "    sharpe_ratio = mean_return / std_return\n",
        "    return f'Trained Sharpe Ratio: {sharpe_ratio}'\n",
        "\n",
        "@dash_app.callback(\n",
        "    Output('update-status', 'children'),\n",
        "    [Input('update-api-button', 'n_clicks')],\n",
        "    [State('api-key', 'value'), State('api-secret', 'value'), State('api-endpoint', 'value')]\n",
        ")\n",
        "def update_api_keys(n_clicks, new_api_key, new_api_secret, new_api_endpoint):\n",
        "    if n_clicks > 0:\n",
        "        os.environ['ALPACA_API_KEY'] = new_api_key\n",
        "        os.environ['ALPACA_SECRET_KEY'] = new_api_secret\n",
        "        os.environ['ALPACA_ENDPOINT'] = new_api_endpoint\n",
        "        return \"API keys updated successfully!\"\n",
        "    return \"\"\n",
        "\n",
        "def within_risk_level(trade, risk_level):\n",
        "    return trade['risk'] <= risk_level\n",
        "\n",
        "def run_dash():\n",
        "    dash_app.run_server(debug=True, use_reloader=False, port=8000)"
      ],
      "metadata": {
        "id": "G4AVRekhtKMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6880c5f-f2ea-480c-d1db-b30ef12eb740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting visualization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile backtesting.py\n",
        "import pandas as pd\n",
        "import backtrader as bt\n",
        "from datetime import datetime\n",
        "from historical_data import fetch_historical_data\n",
        "from preprocessing import preprocess_data\n",
        "from model_preparation import build_lstm_model\n",
        "\n",
        "class LSTMStrategy(bt.Strategy):\n",
        "    def __init__(self):\n",
        "        self.lstm_model = build_lstm_model(input_shape=60)  # Adjust input shape as needed\n",
        "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    def next(self):\n",
        "        data = self.data.close.get(size=60)  # Get the last 60 data points\n",
        "        scaled_data = self.scaler.transform(np.array(data).reshape(-1, 1))\n",
        "        X = np.array([scaled_data])\n",
        "        prediction = self.lstm_model.predict(X)\n",
        "        predicted_price = self.scaler.inverse_transform(prediction)[0][0]\n",
        "\n",
        "        if predicted_price > self.data.close[0]:\n",
        "            self.buy()\n",
        "        else:\n",
        "            self.sell()\n",
        "\n",
        "def perform_backtest(symbol, start_date, end_date):\n",
        "    data = fetch_historical_data(symbol, start_date, end_date)\n",
        "    cerebro = bt.Cerebro()\n",
        "    cerebro.addstrategy(LSTMStrategy)\n",
        "    datafeed = bt.feeds.PandasData(dataname=data)\n",
        "    cerebro.adddata(datafeed)\n",
        "    cerebro.broker.set_cash(100000)\n",
        "    cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe_ratio')\n",
        "    results = cerebro.run()\n",
        "    strat = results[0]\n",
        "    sharpe_ratio = strat.analyzers.sharpe_ratio.get_analysis()['sharperatio']\n",
        "    return f'Sharpe Ratio: {sharpe_ratio}'"
      ],
      "metadata": {
        "id": "z5mjs3R9wFwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee736363-af25-4eee-899f-b1fd714fe882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting backtesting.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hyperparameter_visualization.py\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "from dash import dcc, html, Input, Output\n",
        "import dash\n",
        "\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H3('Hyperparameter Tuning Visualization'),\n",
        "    dcc.Graph(id='hyperparam-tuning-graph'),\n",
        "    html.Button('Train Model', id='train-model-button', n_clicks=0),\n",
        "    html.Div(id='training-status')\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "    Output('hyperparam-tuning-graph', 'figure'),\n",
        "    Input('train-model-button', 'n_clicks'))\n",
        "def update_hyperparam_graph(n_clicks):\n",
        "    if n_clicks > 0:\n",
        "        data = pd.DataFrame({\n",
        "            'param': ['learning_rate', 'dropout', 'units', 'learning_rate', 'dropout', 'units'],\n",
        "            'value': [0.01, 0.2, 50, 0.02, 0.3, 100],\n",
        "            'loss': [0.5, 0.4, 0.3, 0.35, 0.25, 0.2]\n",
        "        })\n",
        "        fig = go.Figure()\n",
        "        for param in data['param'].unique():\n",
        "            param_data = data[data['param'] == param]\n",
        "            fig.add_trace(go.Scatter(x=param_data['value'], y=param_data['loss'], mode='lines+markers', name=param))\n",
        "        fig.update_layout(title='Hyperparameter Tuning Results', xaxis_title='Value', yaxis_title='Loss')\n",
        "        return fig\n",
        "    return go.Figure()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)"
      ],
      "metadata": {
        "id": "l8VrJec3wMKn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ebcb6d7-d93d-4b39-8fa9-aa41654383b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hyperparameter_visualization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile real_time_visualization.py\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "from dash import dcc, html, Input, Output\n",
        "import dash\n",
        "import time\n",
        "import threading\n",
        "\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "data = pd.DataFrame(columns=['time', 'price'])\n",
        "\n",
        "def fetch_real_time_data():\n",
        "    global data\n",
        "    while True:\n",
        "        new_data = {'time': pd.Timestamp.now(), 'price': pd.np.random.rand()}\n",
        "        data = data.append(new_data, ignore_index=True)\n",
        "        time.sleep(1)\n",
        "\n",
        "threading.Thread(target=fetch_real_time_data).start()\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H3('Real-Time Market Analysis'),\n",
        "    dcc.Graph(id='market-analysis-chart'),\n",
        "    dcc.Interval(\n",
        "        id='interval-component',\n",
        "        interval=1*1000,  # in milliseconds (update every 1 second)\n",
        "        n_intervals=0\n",
        "    )\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "    Output('market-analysis-chart', 'figure'),\n",
        "    [Input('interval-component', 'n_intervals')])\n",
        "def update_market_analysis(n):\n",
        "    fig = go.Figure(data=[go.Scatter(x=data['time'], y=data['price'], mode='lines+markers')])\n",
        "    fig.update_layout(title='Real-Time Market Data', xaxis_title='Time', yaxis_title='Price')\n",
        "    return fig\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)"
      ],
      "metadata": {
        "id": "uzQb5HoiwzV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c3ab76-4c95-4f4a-e266-770a67b35232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting real_time_visualization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile performance_reporting.py\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "from dash import dcc, html, Input, Output, State\n",
        "import dash\n",
        "\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H3('Performance Report Generation'),\n",
        "    dcc.Dropdown(\n",
        "        id='report-period',\n",
        "        options=[\n",
        "            {'label': 'Last Day', 'value': '1D'},\n",
        "            {'label': 'Last Week', 'value': '1W'},\n",
        "            {'label': 'Last Month', 'value': '1M'}\n",
        "        ],\n",
        "        value='1D'\n",
        "    ),\n",
        "    html.Button('Generate Report', id='generate-report-button', n_clicks=0),\n",
        "    html.Div(id='report-output'),\n",
        "    dcc.Graph(id='performance-metrics-graph')\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "    [Output('report-output', 'children'), Output('performance-metrics-graph', 'figure')],\n",
        "    [Input('generate-report-button', 'n_clicks')],\n",
        "    [State('report-period', 'value')])\n",
        "def generate_performance_report(n_clicks, period):\n",
        "    if n_clicks > 0:\n",
        "        report_data = pd.DataFrame({\n",
        "            'metric': ['P&L', 'Sharpe Ratio', 'Win Rate'],\n",
        "            'value': [1000, 1.5, 0.6]\n",
        "        })\n",
        "        report_text = f\"Performance Report for Period: {period}\\n\"\n",
        "        report_text += report_data.to_string(index=False)\n",
        "\n",
        "        fig = go.Figure(data=[\n",
        "            go.Bar(x=report_data['metric'], y=report_data['value'])\n",
        "        ])\n",
        "        fig.update_layout(title='Performance Metrics')\n",
        "\n",
        "        return report_text, fig\n",
        "    return \"\", go.Figure()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)"
      ],
      "metadata": {
        "id": "uPY6WIgAxHaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d72a178-1d18-4125-fb12-860b2e5ad847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting performance_reporting.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dashboard_customization.py\n",
        "import dash\n",
        "from dash import dcc, html, Input, Output, State\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "available_metrics = ['P&L', 'Sharpe Ratio', 'Win Rate', 'Volatility', 'Drawdown']\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H3('Customize Dashboard'),\n",
        "    dcc.Checklist(\n",
        "        id='selected-metrics',\n",
        "        options=[{'label': metric, 'value': metric} for metric in available_metrics],\n",
        "        value=['P&L', 'Sharpe Ratio']\n",
        "    ),\n",
        "    html.Button('Save Configuration', id='save-config-button', n_clicks=0),\n",
        "    html.Div(id='config-status'),\n",
        "    dcc.Graph(id='custom-dashboard')\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "    Output('config-status', 'children'),\n",
        "    Input('save-config-button', 'n_clicks'),\n",
        "    State('selected-metrics', 'value'))\n",
        "def save_configuration(n_clicks, selected_metrics):\n",
        "    if n_clicks > 0:\n",
        "        return f\"Configuration saved: {', '.join(selected_metrics)}\"\n",
        "    return \"\"\n",
        "\n",
        "@app.callback(\n",
        "    Output('custom-dashboard', 'figure'),\n",
        "    Input('selected-metrics', 'value'))\n",
        "def update_dashboard(selected_metrics):\n",
        "    data = pd.DataFrame({\n",
        "        'metric': selected_metrics,\n",
        "        'value': [pd.np.random.rand() * 1000 for _ in selected_metrics]\n",
        "    })\n",
        "    fig = go.Figure(data=[\n",
        "        go.Bar(x=data['metric'], y=data['value'])\n",
        "    ])\n",
        "    fig.update_layout(title='Custom Dashboard')\n",
        "    return fig\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)"
      ],
      "metadata": {
        "id": "K6bivCNXxQN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98530993-e18b-44c3-ba1c-d1600fea9698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting dashboard_customization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_pipeline.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from preprocessing import preprocess_data, create_lstm_dataset\n",
        "from genetic_optimization import genetic_algorithm_optimization\n",
        "\n",
        "# Load and preprocess data\n",
        "data = pd.read_csv('historical_data.csv')\n",
        "preprocessed_data, scaler = preprocess_data(data)\n",
        "\n",
        "# Prepare LSTM dataset\n",
        "scaled_close = preprocessed_data['scaled_close'].values.reshape(-1, 1)\n",
        "X, y = create_lstm_dataset(scaled_close)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Optimize hyperparameters using Genetic Algorithm\n",
        "best_params = genetic_algorithm_optimization()\n",
        "\n",
        "# Build and train the LSTM model\n",
        "def build_lstm_model(input_shape, units, dropout_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=True, input_shape=(input_shape, 1)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(LSTM(units=units, return_sequences=False))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(units=25))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "model = build_lstm_model(input_shape=X_train.shape[1], units=best_params[0], dropout_rate=best_params[1])\n",
        "model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=1)\n",
        "model.save('trained_lstm_model.h5')"
      ],
      "metadata": {
        "id": "_I_zfIslxf9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d370cd99-fc07-41ea-ecd0-6bb375fc6274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile genetic_optimization.py\n",
        "import numpy as np\n",
        "from deap import base, creator, tools, algorithms\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the genetic algorithm parameters\n",
        "POPULATION_SIZE = 50\n",
        "GENERATIONS = 30\n",
        "MUTATION_RATE = 0.1\n",
        "CROSSOVER_RATE = 0.5\n",
        "\n",
        "def build_lstm_model(input_shape, units, dropout_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=True, input_shape=(input_shape, 1)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(LSTM(units=units, return_sequences=False))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(units=25))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "def evaluate_model(individual):\n",
        "    units, dropout_rate = individual\n",
        "    model = build_lstm_model(input_shape=X_train.shape[1], units=units, dropout_rate=dropout_rate)\n",
        "    model.fit(X_train, y_train, epochs=10, verbose=0)\n",
        "    loss = model.evaluate(X_val, y_val, verbose=0)\n",
        "    return loss,\n",
        "\n",
        "# DEAP setup\n",
        "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_int\", np.random.randint, 10, 100)\n",
        "toolbox.register(\"attr_float\", np.random.uniform, 0.1, 0.5)\n",
        "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
        "                 (toolbox.attr_int, toolbox.attr_float), n=1)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
        "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"evaluate\", evaluate_model)\n",
        "\n",
        "# Genetic Algorithm\n",
        "def genetic_algorithm_optimization():\n",
        "    population = toolbox.population(n=POPULATION_SIZE)\n",
        "    algorithms.eaSimple(population, toolbox, cxpb=CROSSOVER_RATE, mutpb=MUTATION_RATE, ngen=GENERATIONS, verbose=True)\n",
        "    best_individual = tools.selBest(population, k=1)[0]\n",
        "    return best_individual"
      ],
      "metadata": {
        "id": "LNwnpd3KxpeZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ebd92b-c899-4093-fcde-5b4ab4a3fed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting genetic_optimization.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prediction_pipeline.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "from preprocessing import preprocess_data, create_lstm_dataset\n",
        "\n",
        "# Load trained model\n",
        "model = load_model('trained_lstm_model.h5')\n",
        "\n",
        "def generate_predictions(data):\n",
        "    preprocessed_data, scaler = preprocess_data(data)\n",
        "    scaled_close = preprocessed_data['scaled_close'].values.reshape(-1, 1)\n",
        "    X, y = create_lstm_dataset(scaled_close)\n",
        "    predictions = model.predict(X)\n",
        "    predictions = scaler.inverse_transform(predictions)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "fy-wByiJxwhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd853b5-9970-477c-939c-8ad2cc44e087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting prediction_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Dockerfile\n",
        "# Use an official Python runtime as a parent image\n",
        "FROM python:3.8-slim\n",
        "\n",
        "# Set the working directory\n",
        "WORKDIR /usr/src/app\n",
        "\n",
        "# Copy the current directory contents into the container at /usr/src/app\n",
        "COPY . .\n",
        "\n",
        "# Install any needed packages specified in requirements.txt\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Expose port 8000 for the API\n",
        "EXPOSE 8000\n",
        "\n",
        "# Run FastAPI and Dash using Uvicorn and the provided app module\n",
        "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
      ],
      "metadata": {
        "id": "ryzuKnB1x6a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62bb0784-2f23-455b-eb4a-11a66c685f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "alpaca-trade-api\n",
        "alpha_vantage\n",
        "tensorflow\n",
        "scikit-learn\n",
        "matplotlib\n",
        "dash\n",
        "dash-core-components\n",
        "dash-html-components\n",
        "dash-bootstrap-components\n",
        "fastapi\n",
        "uvicorn\n",
        "nest-asyncio\n",
        "pyngrok\n",
        "transformers\n",
        "plotly\n",
        "pandas\n",
        "requests\n",
        "backtrader\n",
        "boto3\n",
        "keras-tuner\n",
        "deap"
      ],
      "metadata": {
        "id": "GV2v7ynwyIxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9a9837-074a-4c44-ec33-5688849cfa54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_streams.py  # This will start the real-time data fetching"
      ],
      "metadata": {
        "id": "NE7S925ryNyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import unbind_port\n",
        "\n",
        "unbind_port.unbind_port(8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEm3GeJgSeiT",
        "outputId": "d85ee311-4cdd-489c-f71f-3c0e6fe23640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Port 8000 is already free.\n",
            "Port 8000 is already free.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uvicorn\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.responses import HTMLResponse\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "from threading import Thread\n",
        "from visualization import run_dash, dash_app\n",
        "from trade_execution import execute_trade\n",
        "from preprocessing import preprocess_data\n",
        "from model_preparation import build_lstm_model\n",
        "\n",
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    html_content = \"\"\"\n",
        "    <html>\n",
        "        <head>\n",
        "            <title>AutoGPT Trader Dashboard</title>\n",
        "        </head>\n",
        "        <body>\n",
        "            <iframe src=\"/dash/\" style=\"width:100%; height:100vh;\"></iframe>\n",
        "        </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return HTMLResponse(content=html_content)\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "def predict(data: dict):\n",
        "    preprocessed_data, scaler = preprocess_data(data)\n",
        "    X, y = create_lstm_dataset(preprocessed_data['scaled_close'].values.reshape(-1, 1))\n",
        "    lstm_model = build_lstm_model(X.shape[1])\n",
        "    predictions = lstm_model.predict(X)\n",
        "    return {\"predictions\": scaler.inverse_transform(predictions).tolist()}\n",
        "\n",
        "@app.get(\"/dash/\")\n",
        "def get_dash_app(request: Request):\n",
        "    return HTMLResponse(dash_app.index(), status_code=200)\n",
        "\n",
        "# Load Ngrok configuration from the YAML file\n",
        "ngrok_tunnel = ngrok.connect(addr=8000, config_path=\"/root/.ngrok2/ngrok.yml\")\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dash_thread = Thread(target=run_dash)\n",
        "    dash_thread.start()\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, loop=\"asyncio\")"
      ],
      "metadata": {
        "id": "6HuR6DEnyX2I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "outputId": "85525cf3-009b-4a37-fa9e-dd2800bf8220",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-06-29T16:58:32+0000 lvl=warn msg=\"ngrok config file found at both XDG and legacy locations, using XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n",
            "ERROR:pyngrok.process.ngrok:t=2024-06-29T16:58:32+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2024-06-29T16:58:32+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2024-06-29T16:58:32+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2024-06-29T16:58:32+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-dd45e8c52654>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Load Ngrok configuration from the YAML file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mngrok_tunnel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/root/.ngrok2/ngrok.yml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Public URL:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrok_tunnel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublic_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    399\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4YKqopODRCf0Kp3aVsDaP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}