{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZilferII/CodeVault/blob/main/AutoGPT_stocksv5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y build-essential wget libncurses5-dev libncursesw5-dev\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok"
      ],
      "metadata": {
        "id": "RcNWOD2cRtnH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed4e68a2-7a7c-4ddd-96cc-5465348dc799"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (13.224.214.61)] [Connecting to ppa.launc\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                                                    \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [2 InRelease 43.1 kB/128 kB 34%] [3 InRelease 73.5 kB/129 kB 57%] [Connected to cloud.r-project.o\r0% [2 InRelease 75.0 kB/128 kB 59%] [Waiting for headers] [Connecting to ppa.launchpadcontent.net (1\r0% [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.80)] [Waiting for heade\r                                                                                                    \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [Connected to ppa.launchpadcontent.net (185.125.190.80)] [Waiting for heade\r                                                                                                    \rGet:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,566 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,974 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,398 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,639 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,238 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [32.2 kB]\n",
            "Fetched 11.2 MB in 2s (6,264 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libncurses5-dev is already the newest version (6.3-2ubuntu0.1).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "The following NEW packages will be installed:\n",
            "  libncursesw5-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 790 B of archives.\n",
            "After this operation, 6,144 B of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libncursesw5-dev amd64 6.3-2ubuntu0.1 [790 B]\n",
            "Fetched 790 B in 0s (20.7 kB/s)\n",
            "Selecting previously unselected package libncursesw5-dev:amd64.\n",
            "(Reading database ... 121925 files and directories currently installed.)\n",
            "Preparing to unpack .../libncursesw5-dev_6.3-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libncursesw5-dev:amd64 (6.3-2ubuntu0.1) ...\n",
            "Setting up libncursesw5-dev:amd64 (6.3-2ubuntu0.1) ...\n",
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Get:7 https://ngrok-agent.s3.amazonaws.com buster InRelease [20.3 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://ngrok-agent.s3.amazonaws.com buster/main amd64 Packages [4,888 B]\n",
            "Fetched 25.2 kB in 2s (14.2 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "46 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  ngrok\n",
            "0 upgraded, 1 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 6,507 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 https://ngrok-agent.s3.amazonaws.com buster/main amd64 ngrok amd64 3.12.0 [6,507 kB]\n",
            "Fetched 6,507 kB in 1s (7,750 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package ngrok.\n",
            "(Reading database ... 121926 files and directories currently installed.)\n",
            "Preparing to unpack .../ngrok_3.12.0_amd64.deb ...\n",
            "Unpacking ngrok (3.12.0) ...\n",
            "Setting up ngrok (3.12.0) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install alpaca-trade-api alpha_vantage tensorflow scikit-learn matplotlib dash dash-core-components dash-html-components dash-bootstrap-components fastapi uvicorn nest-asyncio pyngrok transformers plotly pandas requests backtrader boto3 keras-tuner deap"
      ],
      "metadata": {
        "id": "dsdt9GXvRvyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['ALPACA_API_KEY'] = 'PKDEDZT5O8CI3R6UTONM'\n",
        "os.environ['ALPACA_SECRET_KEY'] = 'aPHjNjSp7d6gfDeUY1aElMc92iijlvcMUrnlTIUr'\n",
        "os.environ['ALPACA_ENDPOINT'] = 'https://paper-api.alpaca.markets'\n",
        "os.environ['NEWS_API_KEY'] = '8cc36860da3d4624810890fe4feb78e3'\n",
        "os.environ['FMP_API_KEY'] = 'efCfJ6ItVI48a16QgQrc8WxH4BORgQTa'"
      ],
      "metadata": {
        "id": "qPyidTS8R6ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the path for the ngrok configuration file\n",
        "xdg_config_path = os.path.expanduser(\"~/.config/ngrok/ngrok.yml\")\n",
        "\n",
        "# Define the content of the configuration file\n",
        "ngrok_config_content = \"\"\"\n",
        "authtoken: 2iXBVyuqMaMjHyK44ixjAcouHKe_5A2z1GqFXjoaST4Yh7GDQ\n",
        "tunnels:\n",
        "  myservice:\n",
        "    addr: 8000\n",
        "    proto: http\n",
        "\"\"\"\n",
        "\n",
        "# Ensure the directory exists and write the configuration file\n",
        "config_dir = os.path.dirname(xdg_config_path)\n",
        "if not os.path.exists(config_dir):\n",
        "    os.makedirs(config_dir)\n",
        "with open(xdg_config_path, 'w') as config_file:\n",
        "    config_file.write(ngrok_config_content)\n",
        "\n",
        "print(f\"ngrok configuration file created at {xdg_config_path}\")"
      ],
      "metadata": {
        "id": "GydKYWXySBqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config check"
      ],
      "metadata": {
        "id": "voCwjMXJSJoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_streams.py\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from queue import Queue\n",
        "from threading import Thread\n",
        "import time\n",
        "\n",
        "ALPACA_API_KEY = os.getenv('ALPACA_API_KEY')\n",
        "ALPACA_SECRET_KEY = os.getenv('ALPACA_SECRET_KEY')\n",
        "ALPACA_ENDPOINT = os.getenv('ALPACA_ENDPOINT', 'https://paper-api.alpaca.markets')\n",
        "FMP_API_KEY = os.getenv('FMP_API_KEY')\n",
        "\n",
        "data_queue = Queue()\n",
        "\n",
        "def fetch_data(endpoint, params={}):\n",
        "    base_url = f\"https://financialmodelingprep.com/api\"\n",
        "    url = f\"{base_url}{endpoint}?apikey={FMP_API_KEY}\"\n",
        "    for key, value in params.items():\n",
        "        url += f\"&{key}={value}\"\n",
        "    response = requests.get(url)\n",
        "    return response.json()\n",
        "\n",
        "def normalize_data(data):\n",
        "    if isinstance(data, list):\n",
        "        return pd.DataFrame(data)\n",
        "    elif isinstance(data, dict) and 'data' in data:\n",
        "        return pd.DataFrame(data['data'])\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def fetch_alpaca_data(symbol):\n",
        "    import alpaca_trade_api as tradeapi\n",
        "    api = tradeapi.REST(ALPACA_API_KEY, ALPACA_SECRET_KEY, ALPACA_ENDPOINT, api_version='v2')\n",
        "    bars = api.get_bars(symbol, '1D', limit=100)\n",
        "    return [{'time': bar.t.isoformat(), 'open': bar.o, 'high': bar.h, 'low': bar.l, 'close': bar.c, 'volume': bar.v} for bar in bars]\n",
        "\n",
        "def fetch_real_time_data():\n",
        "    while True:\n",
        "        combined_data = {\n",
        "            \"stock_quote\": fetch_data('/v3/quote/AAPL'),\n",
        "            \"historical_prices\": fetch_alpaca_data('AAPL'),\n",
        "            \"news\": fetch_data('/v3/stock_news', {'tickers': 'AAPL', 'limit': 50}),\n",
        "            \"social_sentiment\": fetch_data('/v4/historical/social-sentiment', {'symbol': 'AAPL', 'page': 0}),\n",
        "        }\n",
        "        normalized_data = {key: normalize_data(value) for key, value in combined_data.items()}\n",
        "        data_queue.put(normalized_data)\n",
        "        time.sleep(60)"
      ],
      "metadata": {
        "id": "_laH9bB3SQyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocessing.py\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from feature_engineering import extract_features, analyze_sentiments\n",
        "\n",
        "def preprocess_data(data):\n",
        "    features = extract_features(data)\n",
        "    features = analyze_sentiments(features)\n",
        "\n",
        "    historical_prices = pd.DataFrame(features['historical_prices'])\n",
        "    social_sentiment = pd.DataFrame(features['social_sentiment'])\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    historical_prices['scaled_close'] = scaler.fit_transform(historical_prices[['close']])\n",
        "\n",
        "    merged_data = historical_prices.merge(social_sentiment, left_on='time', right_on='date', how='left')\n",
        "    merged_data.fillna(method='ffill', inplace=True)\n",
        "    return merged_data, scaler\n",
        "\n",
        "def create_lstm_dataset(data, window_size=60):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - window_size - 1):\n",
        "        X.append(data[i:(i + window_size), 0])\n",
        "        y.append(data[i + window_size, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def preprocess_for_random_forest(data):\n",
        "    data = data.dropna()\n",
        "    X = data.drop(columns=['target']).values\n",
        "    y = data['target'].values\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "5ws1dy7nSZXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile feature_engineering.py\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "def extract_features(data):\n",
        "    features = []\n",
        "    for key, df in data.items():\n",
        "        if key == 'historical_prices':\n",
        "            df['close'] = df['close'].astype(float)\n",
        "            features.extend(df[['time', 'close']].to_dict('records'))\n",
        "        elif key == 'social_sentiment':\n",
        "            features.extend(df[['date', 'sentiment']].to_dict('records'))\n",
        "    return features\n",
        "\n",
        "def analyze_sentiments(features):\n",
        "    sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
        "    for feature in features:\n",
        "        if 'sentiment' in feature:\n",
        "            sentiment_result = sentiment_analysis(feature['sentiment'])[0]\n",
        "            feature['sentiment_analysis'] = sentiment_result['label']\n",
        "    return features"
      ],
      "metadata": {
        "id": "MyEKau5LSdOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model_preparation.py\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def build_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=50, return_sequences=True, input_shape=(input_shape, 1)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=50, return_sequences=False))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=25))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "def train_lstm_model(X_train, y_train, X_val, y_val):\n",
        "    model = build_lstm_model(X_train.shape[1])\n",
        "    model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=1)\n",
        "    return model\n",
        "\n",
        "def build_rl_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=input_shape, activation='relu'))\n",
        "    model.add(Dense(24, activation='relu'))\n",
        "    model.add(Dense(2, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "def train_rl_model(model, data, epochs=100):\n",
        "    for epoch in range(epochs):\n",
        "        state = data[0]\n",
        "        for t in range(len(data) - 1):\n",
        "            action = np.argmax(model.predict(state.reshape(1, -1)))\n",
        "            next_state = data[t + 1]\n",
        "            reward = get_reward(next_state, action)\n",
        "            target = reward + 0.99 * np.max(model.predict(next_state.reshape(1, -1)))\n",
        "            target_f = model.predict(state.reshape(1, -1))\n",
        "            target_f[0][action] = target\n",
        "            model.fit(state.reshape(1, -1), target_f, epochs=1, verbose=0)\n",
        "            state = next_state\n",
        "\n",
        "def get_reward(state, action):\n",
        "    price_change = state[1] - state[0]\n",
        "    if action == 1:  # Buy action\n",
        "        return price_change if price_change > 0 else -1\n",
        "    elif action == 0:  # Sell action\n",
        "        return -price_change if price_change < 0 else -1\n",
        "    return -1"
      ],
      "metadata": {
        "id": "JEMXpHU2SefI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile pattern_recognition.py\n",
        "import talib\n",
        "import pandas as pd\n",
        "\n",
        "def recognize_patterns(data):\n",
        "    patterns = []\n",
        "    open_prices = data['open'].values\n",
        "    high_prices = data['high'].values\n",
        "    low_prices = data['low'].values\n",
        "    close_prices = data['close'].values\n",
        "\n",
        "    engulfing = talib.CDLENGULFING(open_prices, high_prices, low_prices, close_prices)\n",
        "\n",
        "    for i in range(len(engulfing)):\n",
        "        if engulfing[i] == 100:  # Bullish engulfing\n",
        "            patterns.append({\n",
        "                'symbol': data['symbol'].iloc[i],\n",
        "                'pattern': 'bullish_engulfing',\n",
        "                'confidence': 0.9,\n",
        "                'index': i\n",
        "            })\n",
        "        elif engulfing[i] == -100:  # Bearish engulfing\n",
        "            patterns.append({\n",
        "                'symbol': data['symbol'].iloc[i],\n",
        "                'pattern': 'bearish_engulfing',\n",
        "                'confidence': 0.9,\n",
        "                'index': i\n",
        "            })\n",
        "    return patterns"
      ],
      "metadata": {
        "id": "atwS7JtTSe4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile trade_execution.py\n",
        "import alpaca_trade_api as tradeapi\n",
        "import os\n",
        "import time\n",
        "\n",
        "ALPACA_API_KEY = os.getenv('ALPACA_API_KEY')\n",
        "ALPACA_SECRET_KEY = os.getenv('ALPACA_SECRET_KEY')\n",
        "ALPACA_ENDPOINT = os.getenv('ALPACA_ENDPOINT', 'https://paper-api.alpaca.markets')\n",
        "\n",
        "api = tradeapi.REST(ALPACA_API_KEY, ALPACA_SECRET_KEY, ALPACA_ENDPOINT, api_version='v2')\n",
        "\n",
        "def execute_trade(action, symbol, qty):\n",
        "    try:\n",
        "        if action == 'buy':\n",
        "            api.submit_order(\n",
        "                symbol=symbol,\n",
        "                qty=qty,\n",
        "                side='buy',\n",
        "                type='market',\n",
        "                time_in_force='gtc'\n",
        "            )\n",
        "        elif action == 'sell':\n",
        "            api.submit_order(\n",
        "                symbol=symbol,\n",
        "                qty=qty,\n",
        "                side='sell',\n",
        "                type='market',\n",
        "                time_in_force='gtc'\n",
        "            )\n",
        "        print(f\"Executed {action} trade for {symbol} ({qty} shares).\")\n",
        "    except Exception as e:\n",
        "        print(f\"Trade execution failed: {e}\")\n",
        "        time.sleep(5)\n",
        "        execute_trade(action, symbol, qty)  # Retry once after a short delay"
      ],
      "metadata": {
        "id": "11FYUf6ISfuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile visualization.py\n",
        "from dash import dcc, html, Dash\n",
        "from dash.dependencies import Input, Output, State\n",
        "import pandas as pd\n",
        "from threading import Thread\n",
        "import plotly.graph_objs as go\n",
        "from data_streams import data_queue\n",
        "from trade_execution import execute_trade\n",
        "from preprocessing import preprocess_data, create_lstm_dataset\n",
        "from model_preparation import build_lstm_model, train_lstm_model\n",
        "\n",
        "dash_app = Dash(__name__)\n",
        "\n",
        "dash_app.layout = html.Div([\n",
        "    html.H1(\"AutoGPT Trader Dashboard\"),\n",
        "    dcc.Dropdown(\n",
        "        id='stock-selector',\n",
        "        options=[{'label': 'AAPL', 'value': 'AAPL'}, {'label': 'GOOG', 'value': 'GOOG'}, {'label': 'TSLA', 'value': 'TSLA'}],\n",
        "        value='AAPL'\n",
        "    ),\n",
        "    dcc.Dropdown(\n",
        "        id='risk-level',\n",
        "        options=[{'label': 'Low', 'value': 1}, {'label': 'Medium', 'value': 2}, {'label': 'High', 'value': 3}],\n",
        "        value=2\n",
        "    ),\n",
        "    dcc.Graph(id='price-chart'),\n",
        "    html.Button('Run Model', id='run-model', n_clicks=0),\n",
        "    html.Button('Backtest', id='backtest-button', n_clicks=0),\n",
        "    html.Button('Train Sharpe', id='sharpe-train-button', n_clicks=0),\n",
        "    html.Div(id='trade-output'),\n",
        "    html.Div(id='backtest-output'),\n",
        "    html.Div(id='sharpe-output'),\n",
        "    html.Div([\n",
        "        html.Label('Alpaca API Key:'),\n",
        "        dcc.Input(id='api-key', type='text', value=''),\n",
        "        html.Label('Alpaca API Secret:'),\n",
        "        dcc.Input(id='api-secret', type='password', value=''),\n",
        "        html.Label('Alpaca Endpoint:'),\n",
        "        dcc.Input(id='api-endpoint', type='text', value=''),\n",
        "        html.Button('Update API Keys', id='update-api-button', n_clicks=0),\n",
        "        html.Div(id='update-status')\n",
        "    ])\n",
        "])\n",
        "\n",
        "@dash_app.callback(\n",
        "    [Output('price-chart', 'figure'), Output('trade-output', 'children')],\n",
        "    [Input('run-model', 'n_clicks'), Input('stock-selector', 'value'), Input('risk-level', 'value')]\n",
        ")\n",
        "def update_graph(n_clicks, stock, risk_level):\n",
        "    if n_clicks > 0:\n",
        "        if not data_queue.empty():\n",
        "            data = data_queue.get()\n",
        "            preprocessed_data, scaler = preprocess_data(data)\n",
        "            X, y = create_lstm_dataset(preprocessed_data['scaled_close'].values.reshape(-1, 1))\n",
        "            lstm_model = build_lstm_model(X.shape[1])\n",
        "            lstm_model.fit(X, y, epochs=50, verbose=1)\n",
        "            predictions = lstm_model.predict(X)\n",
        "            trades = [{'symbol': stock, 'profit_margin': 0.05, 'risk': 50}]\n",
        "            execute_trade('buy', stock, 10)  # Replace with actual trading logic\n",
        "            fig = {\n",
        "                'data': [{'x': preprocessed_data['time'], 'y': preprocessed_data['close'], 'type': 'line'}],\n",
        "                'layout': {'title': f'Historical Prices for {stock}'}\n",
        "            }\n",
        "            return fig, \"Trade executed based on model predictions and risk level\"\n",
        "        return {}, \"No data available\"\n",
        "    return {}, \"Click 'Run Model' to execute trading logic\"\n",
        "\n",
        "@dash_app.callback(\n",
        "    Output('backtest-output', 'children'),\n",
        "    [Input('backtest-button', 'n_clicks')],\n",
        "    [Input('stock-selector', 'value')]\n",
        ")\n",
        "def backtest_strategy(n_clicks, stock):\n",
        "    if n_clicks > 0:\n",
        "        backtest_results = perform_backtest(stock, '2020-01-01', '2021-01-01')\n",
        "        return f\"Backtest Results for {stock}: {backtest_results}\"\n",
        "    return \"Click 'Backtest' to run the strategy\"\n",
        "\n",
        "def perform_backtest(symbol, start_date, end_date):\n",
        "    cerebro = bt.Cerebro()\n",
        "    cerebro.addstrategy(LSTMStrategy)\n",
        "    historical_prices = fetch_historical_data(symbol, start_date, end_date)\n",
        "    df = pd.DataFrame(historical_prices)\n",
        "    data = bt.feeds.PandasData(dataname=df)\n",
        "    cerebro.adddata(data)\n",
        "    cerebro.broker.set_cash(100000)\n",
        "    cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe_ratio')\n",
        "    results = cerebro.run()\n",
        "    strat = results[0]\n",
        "    sharpe_ratio = strat.analyzers.sharpe_ratio.get_analysis()['sharperatio']\n",
        "    return f'Sharpe Ratio: {sharpe_ratio}'\n",
        "\n",
        "@dash_app.callback(\n",
        "    Output('sharpe-output', 'children'),\n",
        "    [Input('sharpe-train-button', 'n_clicks')],\n",
        "    [Input('stock-selector', 'value')]\n",
        ")\n",
        "def train_sharpe_strategy(n_clicks, stock):\n",
        "    if n_clicks > 0:\n",
        "        training_results = train_sharpe(stock)\n",
        "        return f\"Sharpe Training Results for {stock}: {training_results}\"\n",
        "    return \"Click 'Train Sharpe' to start the training\"\n",
        "\n",
        "def train_sharpe(stock):\n",
        "    historical_prices = fetch_historical_data(stock, '2020-01-01', '2021-01-01')\n",
        "    df = pd.DataFrame(historical_prices)\n",
        "    returns = df['close'].pct_change().dropna()\n",
        "    mean_return = returns.mean()\n",
        "    std_return = returns.std()\n",
        "    sharpe_ratio = mean_return / std_return\n",
        "    return f'Trained Sharpe Ratio: {sharpe_ratio}'\n",
        "\n",
        "@dash_app.callback(\n",
        "    Output('update-status', 'children'),\n",
        "    [Input('update-api-button', 'n_clicks')],\n",
        "    [State('api-key', 'value'), State('api-secret', 'value'), State('api-endpoint', 'value')]\n",
        ")\n",
        "def update_api_keys(n_clicks, new_api_key, new_api_secret, new_api_endpoint):\n",
        "    if n_clicks > 0:\n",
        "        os.environ['ALPACA_API_KEY'] = new_api_key\n",
        "        os.environ['ALPACA_SECRET_KEY'] = new_api_secret\n",
        "        os.environ['ALPACA_ENDPOINT'] = new_api_endpoint\n",
        "        return \"API keys updated successfully!\"\n",
        "    return \"\"\n",
        "\n",
        "def within_risk_level(trade, risk_level):\n",
        "    return trade['risk'] <= risk_level\n",
        "\n",
        "def run_dash():\n",
        "    dash_app.run_server(debug=True, use_reloader=False, port=8000)"
      ],
      "metadata": {
        "id": "UdlUqypYShFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile backtesting.py\n",
        "import pandas as pd\n",
        "import backtrader as bt\n",
        "from datetime import datetime\n",
        "from historical_data import fetch_historical_data\n",
        "from preprocessing import preprocess_data\n",
        "from model_preparation import build_lstm_model\n",
        "\n",
        "class LSTMStrategy(bt.Strategy):\n",
        "    def __init__(self):\n",
        "        self.lstm_model = build_lstm_model(input_shape=60)  # Adjust input shape as needed\n",
        "        self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    def next(self):\n",
        "        data = self.data.close.get(size=60)  # Get the last 60 data points\n",
        "        scaled_data = self.scaler.transform(np.array(data).reshape(-1, 1))\n",
        "        X = np.array([scaled_data])\n",
        "        prediction = self.lstm_model.predict(X)\n",
        "        predicted_price = self.scaler.inverse_transform(prediction)[0][0]\n",
        "\n",
        "        if predicted_price > self.data.close[0]:\n",
        "            self.buy()\n",
        "        else:\n",
        "            self.sell()\n",
        "\n",
        "def perform_backtest(symbol, start_date, end_date):\n",
        "    data = fetch_historical_data(symbol, start_date, end_date)\n",
        "    cerebro = bt.Cerebro()\n",
        "    cerebro.addstrategy(LSTMStrategy)\n",
        "    datafeed = bt.feeds.PandasData(dataname=data)\n",
        "    cerebro.adddata(datafeed)\n",
        "    cerebro.broker.set_cash(100000)\n",
        "    cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe_ratio')\n",
        "    results = cerebro.run()\n",
        "    strat = results[0]\n",
        "    sharpe_ratio = strat.analyzers.sharpe_ratio.get_analysis()['sharperatio']\n",
        "    return f'Sharpe Ratio: {sharpe_ratio}'"
      ],
      "metadata": {
        "id": "dYMAcwVmSiu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hyperparameter_visualization.py\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "from dash import dcc, html, Input, Output\n",
        "import dash\n",
        "\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H3('Hyperparameter Tuning Visualization'),\n",
        "    dcc.Graph(id='hyperparam-tuning-graph'),\n",
        "    html.Button('Train Model', id='train-model-button', n_clicks=0),\n",
        "    html.Div(id='training-status')\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "    Output('hyperparam-tuning-graph', 'figure'),\n",
        "    Input('train-model-button', 'n_clicks'))\n",
        "def update_hyperparam_graph(n_clicks):\n",
        "    if n_clicks > 0:\n",
        "        data = pd.DataFrame({\n",
        "            'param': ['learning_rate', 'dropout', 'units', 'learning_rate', 'dropout', 'units'],\n",
        "            'value': [0.01, 0.2, 50, 0.02, 0.3, 100],\n",
        "            'loss': [0.5, 0.4, 0.3, 0.35, 0.25, 0.2]\n",
        "        })\n",
        "        fig = go.Figure()\n",
        "        for param in data['param'].unique():\n",
        "            param_data = data[data['param'] == param]\n",
        "            fig.add_trace(go.Scatter(x=param_data['value'], y=param_data['loss'], mode='lines+markers', name=param))\n",
        "        fig.update_layout(title='Hyperparameter Tuning Results', xaxis_title='Value', yaxis_title='Loss')\n",
        "        return fig\n",
        "    return go.Figure()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)"
      ],
      "metadata": {
        "id": "FY7h-QSiSjWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile real_time_visualization.py\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "from dash import dcc, html, Input, Output\n",
        "import dash\n",
        "import time\n",
        "import threading\n",
        "\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "data = pd.DataFrame(columns=['time', 'price'])\n",
        "\n",
        "def fetch_real_time_data():\n",
        "    global data\n",
        "    while True:\n",
        "        new_data = {'time': pd.Timestamp.now(), 'price': pd.np.random.rand()}\n",
        "        data = data.append(new_data, ignore_index=True)\n",
        "        time.sleep(1)\n",
        "\n",
        "threading.Thread(target=fetch_real_time_data).start()\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H3('Real-Time Market Analysis'),\n",
        "    dcc.Graph(id='market-analysis-chart'),\n",
        "    dcc.Interval(\n",
        "        id='interval-component',\n",
        "        interval=1*1000,  # in milliseconds (update every 1 second)\n",
        "        n_intervals=0\n",
        "    )\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "    Output('market-analysis-chart', 'figure'),\n",
        "    [Input('interval-component', 'n_intervals')])\n",
        "def update_market_analysis(n):\n",
        "    fig = go.Figure(data=[go.Scatter(x=data['time'], y=data['price'], mode='lines+markers')])\n",
        "    fig.update_layout(title='Real-Time Market Data', xaxis_title='Time', yaxis_title='Price')\n",
        "    return fig\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)"
      ],
      "metadata": {
        "id": "-ittWSFESk71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile performance_reporting.py\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "from dash import dcc, html, Input, Output, State\n",
        "import dash\n",
        "\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H3('Performance Report Generation'),\n",
        "    dcc.Dropdown(\n",
        "        id='report-period',\n",
        "        options=[\n",
        "            {'label': 'Last Day', 'value': '1D'},\n",
        "            {'label': 'Last Week', 'value': '1W'},\n",
        "            {'label': 'Last Month', 'value': '1M'}\n",
        "        ],\n",
        "        value='1D'\n",
        "    ),\n",
        "    html.Button('Generate Report', id='generate-report-button', n_clicks=0),\n",
        "    html.Div(id='report-output'),\n",
        "    dcc.Graph(id='performance-metrics-graph')\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "    [Output('report-output', 'children'), Output('performance-metrics-graph', 'figure')],\n",
        "    [Input('generate-report-button', 'n_clicks')],\n",
        "    [State('report-period', 'value')])\n",
        "def generate_performance_report(n_clicks, period):\n",
        "    if n_clicks > 0:\n",
        "        report_data = pd.DataFrame({\n",
        "            'metric': ['P&L', 'Sharpe Ratio', 'Win Rate'],\n",
        "            'value': [1000, 1.5, 0.6]\n",
        "        })\n",
        "        report_text = f\"Performance Report for Period: {period}\\n\"\n",
        "        report_text += report_data.to_string(index=False)\n",
        "\n",
        "        fig = go.Figure(data=[\n",
        "            go.Bar(x=report_data['metric'], y=report_data['value'])\n",
        "        ])\n",
        "        fig.update_layout(title='Performance Metrics')\n",
        "\n",
        "        return report_text, fig\n",
        "    return \"\", go.Figure()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)"
      ],
      "metadata": {
        "id": "MQid8FBHW_Qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dashboard_customization.py\n",
        "import dash\n",
        "from dash import dcc, html, Input, Output, State\n",
        "import pandas as pd\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "available_metrics = ['P&L', 'Sharpe Ratio', 'Win Rate', 'Volatility', 'Drawdown']\n",
        "\n",
        "app.layout = html.Div([\n",
        "    html.H3('Customize Dashboard'),\n",
        "    dcc.Checklist(\n",
        "        id='selected-metrics',\n",
        "        options=[{'label': metric, 'value': metric} for metric in available_metrics],\n",
        "        value=['P&L', 'Sharpe Ratio']\n",
        "    ),\n",
        "    html.Button('Save Configuration', id='save-config-button', n_clicks=0),\n",
        "    html.Div(id='config-status'),\n",
        "    dcc.Graph(id='custom-dashboard')\n",
        "])\n",
        "\n",
        "@app.callback(\n",
        "    Output('config-status', 'children'),\n",
        "    Input('save-config-button', 'n_clicks'),\n",
        "    State('selected-metrics', 'value'))\n",
        "def save_configuration(n_clicks, selected_metrics):\n",
        "    if n_clicks > 0:\n",
        "        return f\"Configuration saved: {', '.join(selected_metrics)}\"\n",
        "    return \"\"\n",
        "\n",
        "@app.callback(\n",
        "    Output('custom-dashboard', 'figure'),\n",
        "    Input('selected-metrics', 'value'))\n",
        "def update_dashboard(selected_metrics):\n",
        "    data = pd.DataFrame({\n",
        "        'metric': selected_metrics,\n",
        "        'value': [pd.np.random.rand() * 1000 for _ in selected_metrics]\n",
        "    })\n",
        "    fig = go.Figure(data=[\n",
        "        go.Bar(x=data['metric'], y=data['value'])\n",
        "    ])\n",
        "    fig.update_layout(title='Custom Dashboard')\n",
        "    return fig\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)"
      ],
      "metadata": {
        "id": "2f7tmxVRXAjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_pipeline.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from preprocessing import preprocess_data, create_lstm_dataset\n",
        "from genetic_optimization import genetic_algorithm_optimization\n",
        "\n",
        "# Load and preprocess data\n",
        "data = pd.read_csv('historical_data.csv')\n",
        "preprocessed_data, scaler = preprocess_data(data)\n",
        "\n",
        "# Prepare LSTM dataset\n",
        "scaled_close = preprocessed_data['scaled_close'].values.reshape(-1, 1)\n",
        "X, y = create_lstm_dataset(scaled_close)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Optimize hyperparameters using Genetic Algorithm\n",
        "best_params = genetic_algorithm_optimization()\n",
        "\n",
        "# Build and train the LSTM model\n",
        "def build_lstm_model(input_shape, units, dropout_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=True, input_shape=(input_shape, 1)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(LSTM(units=units, return_sequences=False))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(units=25))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "model = build_lstm_model(input_shape=X_train.shape[1], units=best_params[0], dropout_rate=best_params[1])\n",
        "model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=1)\n",
        "model.save('trained_lstm_model.h5')"
      ],
      "metadata": {
        "id": "ka7SaW6gXBOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile genetic_optimization.py\n",
        "import numpy as np\n",
        "from deap import base, creator, tools, algorithms\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the genetic algorithm parameters\n",
        "POPULATION_SIZE = 50\n",
        "GENERATIONS = 30\n",
        "MUTATION_RATE = 0.1\n",
        "CROSSOVER_RATE = 0.5\n",
        "\n",
        "def build_lstm_model(input_shape, units, dropout_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=True, input_shape=(input_shape, 1)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(LSTM(units=units, return_sequences=False))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(units=25))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "def evaluate_model(individual):\n",
        "    units, dropout_rate = individual\n",
        "    model = build_lstm_model(input_shape=X_train.shape[1], units=units, dropout_rate=dropout_rate)\n",
        "    model.fit(X_train, y_train, epochs=10, verbose=0)\n",
        "    loss = model.evaluate(X_val, y_val, verbose=0)\n",
        "    return loss,\n",
        "\n",
        "# DEAP setup\n",
        "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
        "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
        "\n",
        "toolbox = base.Toolbox()\n",
        "toolbox.register(\"attr_int\", np.random.randint, 10, 100)\n",
        "toolbox.register(\"attr_float\", np.random.uniform, 0.1, 0.5)\n",
        "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
        "                 (toolbox.attr_int, toolbox.attr_float), n=1)\n",
        "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
        "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
        "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.2)\n",
        "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
        "toolbox.register(\"evaluate\", evaluate_model)\n",
        "\n",
        "# Genetic Algorithm\n",
        "def genetic_algorithm_optimization():\n",
        "    population = toolbox.population(n=POPULATION_SIZE)\n",
        "    algorithms.eaSimple(population, toolbox, cxpb=CROSSOVER_RATE, mutpb=MUTATION_RATE, ngen=GENERATIONS, verbose=True)\n",
        "    best_individual = tools.selBest(population, k=1)[0]\n",
        "    return best_individual"
      ],
      "metadata": {
        "id": "Bw5moceyXCdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prediction_pipeline.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "from preprocessing import preprocess_data, create_lstm_dataset\n",
        "\n",
        "# Load trained model\n",
        "model = load_model('trained_lstm_model.h5')\n",
        "\n",
        "def generate_predictions(data):\n",
        "    preprocessed_data, scaler = preprocess_data(data)\n",
        "    scaled_close = preprocessed_data['scaled_close'].values.reshape(-1, 1)\n",
        "    X, y = create_lstm_dataset(scaled_close)\n",
        "    predictions = model.predict(X)\n",
        "    predictions = scaler.inverse_transform(predictions)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "8Tr0qQbLXDNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Dockerfile\n",
        "# Use the official lightweight Python image.\n",
        "# https://hub.docker.com/_/python\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Copy local code to the container image.\n",
        "ENV APP_HOME /app\n",
        "WORKDIR $APP_HOME\n",
        "COPY . ./\n",
        "\n",
        "# Install production dependencies.\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Run the web service on container startup.\n",
        "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
      ],
      "metadata": {
        "id": "ZT9Rk_CIXEDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "alpaca-trade-api\n",
        "alpha_vantage\n",
        "tensorflow\n",
        "scikit-learn\n",
        "matplotlib\n",
        "dash\n",
        "dash-core-components\n",
        "dash-html-components\n",
        "dash-bootstrap-components\n",
        "fastapi\n",
        "uvicorn\n",
        "nest-asyncio\n",
        "pyngrok\n",
        "transformers\n",
        "plotly\n",
        "pandas\n",
        "requests\n",
        "backtrader\n",
        "boto3\n",
        "keras-tuner\n",
        "deap"
      ],
      "metadata": {
        "id": "cOqofdmeXFL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!docker build -t autogpt-trader .\n",
        "!docker run -p 8000:8000 autogpt-trader"
      ],
      "metadata": {
        "id": "INyA2PmPYkH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python data_streams.py  # This will start the real-time data fetching"
      ],
      "metadata": {
        "id": "48KJccPZYlyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uvicorn\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.responses import HTMLResponse\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "from threading import Thread\n",
        "from visualization import run_dash, dash_app\n",
        "from trade_execution import execute_trade\n",
        "from preprocessing import preprocess_data\n",
        "from model_preparation import build_lstm_model\n",
        "\n",
        "nest_asyncio.apply()\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "    html_content = \"\"\"\n",
        "    <html>\n",
        "        <head>\n",
        "            <title>AutoGPT Trader Dashboard</title>\n",
        "        </head>\n",
        "        <body>\n",
        "            <iframe src=\"/dash/\" style=\"width:100%; height:100vh;\"></iframe>\n",
        "        </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return HTMLResponse(content=html_content)\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "def predict(data: dict):\n",
        "    preprocessed_data, scaler = preprocess_data(data)\n",
        "    X, y = create_lstm_dataset(preprocessed_data['scaled_close'].values.reshape(-1, 1))\n",
        "    lstm_model = build_lstm_model(X.shape[1])\n",
        "    predictions = lstm_model.predict(X)\n",
        "    return {\"predictions\": scaler.inverse_transform(predictions).tolist()}\n",
        "\n",
        "@app.get(\"/dash/\")\n",
        "def get_dash_app(request: Request):\n",
        "    return HTMLResponse(dash_app.index(), status_code=200)\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dash_thread = Thread(target=run_dash)\n",
        "    dash_thread.start()\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, loop=\"asyncio\")"
      ],
      "metadata": {
        "id": "weRjWP3YY8QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from threading import Thread\n",
        "\n",
        "def monitor_ngrok_tunnel():\n",
        "    while True:\n",
        "        tunnels = ngrok.get_tunnels()\n",
        "        if not tunnels:\n",
        "            print(\"No active tunnels found. Reconnecting...\")\n",
        "            ngrok_tunnel = ngrok.connect(name=\"myservice\", config_path=ngrok_config_path)\n",
        "            print(\"Reconnected. Public URL:\", ngrok_tunnel.public_url)\n",
        "        else:\n",
        "            print(\"Tunnel is active:\", tunnels[0].public_url)\n",
        "        time.sleep(60)\n",
        "\n",
        "# Start monitoring in a separate thread\n",
        "monitor_thread = Thread(target=monitor_ngrok_tunnel)\n",
        "monitor_thread.start()"
      ],
      "metadata": {
        "id": "NiijjA8wY88F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQV5F9J/PmHmW/23NcahY7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}